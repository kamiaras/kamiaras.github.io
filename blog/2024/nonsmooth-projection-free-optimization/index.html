<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>layout: post title: Nonsmooth Projection-Free Optimization date: 2024-02-01 00:00:00 description: Convex, L-Lipschitz objectives via a projection-free primal–dual saddle formulation and optimistic OMD. tags: optimization convex nonsmooth frank-wolfe saddle-point omd lipschitz categories: research-notes</p> <p><strong>Author:</strong> Kamiar</p> <hr> <h2 id="setup">Setup</h2> <p>Let $\mathcal{X}\subseteq\mathbb{R}^n$ be nonempty, compact, and convex with diameter $D_{\mathcal X}$. Let $\mathcal{Y}\subseteq\mathbb{R}^n$ be closed and convex with $\mathcal{X}\subseteq\mathcal{Y}$; if $\mathcal{Y}$ is bounded, denote its diameter by $D_{\mathcal Y}$. Let $f:\mathcal{Y}\to\mathbb{R}$ be convex and $L$-Lipschitz (w.r.t. $|\cdot|$), i.e., $|g|\le L$ for all $g\in \partial f(y)$ and $y\in\mathcal{Y}$.</p> <p><strong>(P1)</strong></p> \[\min_{x\in\mathcal{X}} f(x).\] <p>Introduce a copy variable $y\in\mathcal{Y}$:</p> <p><strong>(P1.2)</strong></p> \[\min_{x\in\mathcal{X},\,y\in\mathcal{Y}} f(y)\quad \text{s.t.}\quad x=y.\] <p><strong>Lemma (Exact penalty via Lipschitzness).</strong> For any $L$-Lipschitz convex $f$,</p> <p><strong>(P2)</strong></p> \[\min_{x\in\mathcal{X},\,y\in\mathcal{Y}} f(y)+L\|y-x\| \;=\; \min_{x\in\mathcal{X}} f(x).\] <p><em>Proof.</em> For any $x,y$, $f(x)\le f(y)+L|x-y|$, hence the RHS is $\le$ the LHS. Taking $y=x\in\arg\min_{x\in\mathcal{X}} f(x)$ gives equality. □</p> <p>Using the indicator $\delta_{\mathcal{X}}(x)=0$ if $x\in\mathcal{X}$ and $+\infty$ otherwise, (P2) becomes</p> \[\min_{x\in\mathbb{R}^n,\,y\in\mathcal{Y}} f(y)+\delta_{\mathcal{X}}(x)+L\|y-x\|.\] <p>Fenchel representations:</p> \[\delta_{\mathcal{X}}(x)=\sup_{q\in\mathbb{R}^n}\, \langle q, x\rangle-\sigma_{\mathcal{X}}(q), \qquad \|z\|=\sup_{p\in\mathbb{R}^n}\, \langle p, z\rangle-\delta_{\mathcal{B}^*_1}(p),\] <p>where $\sigma_{\mathcal{X}}(q)=\sup_{x\in\mathcal{X}}\langle q,x\rangle$ is the support function and $\mathcal{B}^<em>_r={p:|p|^</em>\le r}$ is the dual-norm ball of radius $r$.</p> <p>Substituting yields a saddle problem:</p> \[\min_{x\in\mathbb{R}^n,\,y\in\mathcal{Y}}\;\max_{q\in\mathbb{R}^n,\,p\in\mathbb{R}^n}\; f(y) + \langle q,x\rangle - \sigma_{\mathcal{X}}(q) + L\langle p, y-x\rangle - L\,\delta_{\mathcal{B}^*_1}(p).\] <p>Minimizing over $x$ enforces $q=Lp$; eliminating $p$ gives $q\in \mathcal{B}^*_L$. Hence:</p> <p><strong>(SP)</strong></p> \[\min_{y\in\mathcal{Y}}\;\max_{q\in \mathcal{B}^*_L}\; f(y) + \langle q, y\rangle - \sigma_{\mathcal{X}}(q).\] <p><strong>Support points.</strong> For $q\in\mathbb{R}^n$, let $s_{\mathcal{X}}(q)\in\arg\max_{x\in\mathcal{X}}\langle q,x\rangle$ be any <em>support point</em> (a subgradient of $\sigma_{\mathcal{X}}$ at $q$). Then $\langle q, s_{\mathcal{X}}(q)-x\rangle\ge 0$ for all $x\in\mathcal{X}$.</p> <hr> <h2 id="algorithm-1--basic-primaldual-scheme-euclidean-case">Algorithm 1 — Basic primal–dual scheme (Euclidean case)</h2> <p>Assume $|\cdot|=|\cdot|_2$. With stepsizes $\eta,\alpha&gt;0$ and subgradients $g_t\in\partial f(y_t)$,</p> \[y_{t+1} = \arg\min_{y\in\mathcal{Y}} \; \langle g_t+q_t,\,y\rangle+\tfrac{\eta}{2}\|y-y_t\|_2^2,\] \[q_{t+1} = \arg\min_{q\in \mathcal{B}^*_L}\; \langle q,\,s_{\mathcal{X}}(q_t)-y_t\rangle+\tfrac{\alpha}{2}\|q-q_t\|_2^2.\] <p><em>Note.</em> One may drop the explicit constraint $q\in \mathcal{B}^*_L$ if regularization implicitly keeps $q_t$ bounded.</p> <h3 id="analysis-sketch">Analysis (Sketch)</h3> <p>For any $x_<em>\in\mathcal{X}$ and $q_</em>\in \mathcal{B}^*_L$,</p> \[\sum_{t=1}^T\langle g_t, y_t-x_*\rangle + \sum_{t=1}^T \langle q_*, y_t-s_{\mathcal{X}}(q_t)\rangle \le \frac{T L^2}{\eta}+\frac{\eta D_{\mathcal X}^2}{2} +\frac{T D_{\mathcal Y}^2}{2\alpha}+\frac{\alpha L^2}{2}.\] <p>With $\bar y_T=\tfrac1T\sum_{t=1}^T y_t$, $\bar x_T=\tfrac1T\sum_{t=1}^T s_{\mathcal{X}}(q_t)$, and $q_<em>\in\partial f(\bar x_T)$ ($|q_</em>|\le L$),</p> \[f(\bar x_T)-f(x_*) \le \frac{L^2}{\eta} +\frac{\eta D_{\mathcal X}^2}{2T} +\frac{D_{\mathcal Y}^2}{2\alpha} +\frac{\alpha L^2}{2T}.\] <p><strong>Corollary (Rate for Algorithm 1).</strong> With $\eta=\sqrt{2}\,L\sqrt{T}/D_{\mathcal X}$ and $\alpha=\sqrt{T}\,D_{\mathcal Y}/L$,</p> \[f(\bar x_T)-f(x_*) \le \frac{(\sqrt{2}\,D_{\mathcal X} + D_{\mathcal Y})\,L}{\sqrt{T}}.\] <hr> <h2 id="algorithm-2--replace-d_mathcal-y-by-d_mathcal-x-via-optimistic-omd">Algorithm 2 — Replace $D_{\mathcal Y}$ by $D_{\mathcal X}$ via optimistic OMD</h2> <p>Initialize $y_0\in\mathcal{X}$, set $q_0=0$ so that $s_{\mathcal{X}}(q_0)=y_0$, and take $q_1=q_0$, $y_1=y_0$. For $t\ge 1$, with $g_t\in\partial f(y_t)$,</p> \[y_{t+1} = \arg\min_{y\in\mathcal{Y}} \; \langle g_t+q_t,\,y\rangle+\tfrac{\eta}{2}\|y-y_t\|_2^2,\] \[q_{t+1} = \arg\min_{q\in \mathcal{B}^*_L}\; \left\langle q,\;2s_{\mathcal{X}}(q_t)-s_{\mathcal{X}}(q_{t-1})-y_{t+1}\right\rangle +\tfrac{\alpha}{2}\|q-q_t\|_2^2.\] <p>This is an <em>optimistic</em> OMD step for $g^{(q)}<em>t := s</em>{\mathcal{X}}(q_t)-y_t$ with prediction $\tilde g^{(q)}<em>t := s</em>{\mathcal{X}}(q_{t-1})-y_t$.</p> <h3 id="analysis-sketch-1">Analysis (Sketch)</h3> <p>For any $q_<em>\in\mathcal{B}^</em>_L$,</p> \[\sum_{t=1}^T \langle q_t-q_*,\,s_{\mathcal{X}}(q_t)-y_t\rangle \le \sum_{t=1}^T \frac{\|s_{\mathcal{X}}(q_t)-s_{\mathcal{X}}(q_{t-1})\|_2^2}{2\alpha} +\frac{\alpha}{2}\|q_*-q_1\|_2^2.\] <p>Since $s_{\mathcal{X}}(q_t)\in\mathcal{X}$ and $q_1=0$,</p> \[\sum_{t=1}^T \langle q_t-q_*,\,s_{\mathcal{X}}(q_t)-y_t\rangle \le \frac{T\,D_{\mathcal X}^2}{2\alpha} + \frac{\alpha L^2}{2}.\] <p>Combining with the $y$-update bound (using $D_{\mathcal X}$ for both telescoping terms),</p> \[f(\bar x_T)-f(x_*) \le \frac{L^2}{\eta} +\frac{\eta D_{\mathcal X}^2}{2T} +\frac{D_{\mathcal X}^2}{2\alpha} +\frac{\alpha L^2}{2T}.\] <p><strong>Corollary (Rate for Algorithm 2).</strong> With $\eta=\sqrt{2}\,L\sqrt{T}/D_{\mathcal X}$ and $\alpha=\sqrt{T}\,D_{\mathcal X}/L$,</p> \[f(\bar x_T)-f(x_*) \le \frac{(\sqrt{2}+1)\,L\,D_{\mathcal X}}{\sqrt{T}}.\] <p><strong>Remark (Oracle model).</strong> The update $s_{\mathcal{X}}(q_t)\in\arg\max_{x\in\mathcal{X}}\langle q_t,x\rangle$ is a linear minimization oracle (LMO) as in Frank–Wolfe, so the scheme is projection-free on $\mathcal{X}$.</p> <hr> <h2 id="references">References</h2> <ul> <li>Orabona, F. (2023). <em>A Modern Introduction to Online Learning</em>. arXiv preprint.</li> </ul> </body></html>