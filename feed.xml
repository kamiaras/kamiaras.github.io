<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kamiaras.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kamiaras.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-10T01:13:34+00:00</updated><id>https://kamiaras.github.io/feed.xml</id><title type="html">Kamy’s Website</title><subtitle>A PhD student&apos;s platform for sharing research and passions. </subtitle><entry><title type="html">Nonsmooth Projection-Free Optimization</title><link href="https://kamiaras.github.io/blog/2024/nonsmooth-projection-free-optimization/" rel="alternate" type="text/html" title="Nonsmooth Projection-Free Optimization"/><published>2024-02-01T00:00:00+00:00</published><updated>2024-02-01T00:00:00+00:00</updated><id>https://kamiaras.github.io/blog/2024/nonsmooth-projection-free-optimization</id><content type="html" xml:base="https://kamiaras.github.io/blog/2024/nonsmooth-projection-free-optimization/"><![CDATA[<p><strong>Author:</strong> Kamiar<br/> <strong>Date:</strong> February 2024</p> <h2 id="setup">Setup</h2> <p>Let $\mathcal{X}\subseteq\mathbb{R}^n$ be nonempty, compact, and convex with diameter $D_{\mathcal X}$. Let $\mathcal{Y}\subseteq\mathbb{R}^n$ be a closed, convex set with $\mathcal{X}\subseteq\mathcal{Y}$; if $\mathcal{Y}$ is bounded, denote its diameter by $D_{\mathcal Y}$. Let $f:\mathcal{Y}\to\mathbb{R}$ be convex and $L$-Lipschitz (with respect to $|\cdot|$), i.e., $|g|\le L$ for all $g\in \partial f(y)$ and $y\in\mathcal{Y}$.</p> <p>Our goal is</p> <p><strong>(P1)</strong><br/> [ \min_{x\in\mathcal{X}} f(x). ]</p> <p>Introduce a copy variable $y\in\mathcal{Y}$ and rewrite</p> <p><strong>(P1.2)</strong><br/> [ \min_{x\in\mathcal{X},\,y\in\mathcal{Y}} f(y)\quad \text{s.t.}\quad x=y. ]</p> <p><strong>Lemma (Exact penalty via Lipschitzness).</strong> For any $L$-Lipschitz convex $f$, <strong>(P2)</strong><br/> [ \min_{x\in\mathcal{X},\,y\in\mathcal{Y}} f(y)+L|y-x| \;=\; \min_{x\in\mathcal{X}} f(x). ]</p> <p><em>Proof.</em> For any $x,y$, $f(x)\le f(y)+L|x-y|$, hence the RHS is $\le$ the LHS. Taking $y=x\in\arg\min_{x\in\mathcal{X}} f(x)$ gives equality. $\square$</p> <p>Using the indicator $\delta_{\mathcal{X}}(x)=0$ if $x\in\mathcal{X}$ and $+\infty$ otherwise, (P2) becomes [ \min_{x\in\mathbb{R}^n,\,y\in\mathcal{Y}} f(y)+\delta_{\mathcal{X}}(x)+L|y-x|. ]</p> <p>Recall the Fenchel representations [ \delta_{\mathcal{X}}(x)=\sup_{q\in\mathbb{R}^n}\, \langle q, x\rangle-\sigma_{\mathcal{X}}(q), \qquad |z|=\sup_{p\in\mathbb{R}^n}\, \langle p, z\rangle-\delta_{\mathcal{B}^<em>_1}(p), ] where $\sigma_{\mathcal{X}}(q)=\sup_{x\in\mathcal{X}}\langle q,x\rangle$ is the support function and $\mathcal{B}^</em><em>r={p:|p|^*\le r}$ is the dual-norm ball of radius $r$. Substituting yields the saddle formulation [ \min</em>{x\in\mathbb{R}^n,\,y\in\mathcal{Y}}\;\max_{q\in\mathbb{R}^n,\,p\in\mathbb{R}^n}\; f(y) + \langle q,x\rangle - \sigma_{\mathcal{X}}(q) + L\langle p, y-x\rangle - L\,\delta_{\mathcal{B}^*_1}(p). ]</p> <p>The minimization over $x$ enforces $q=Lp$; eliminating $p$ gives $q\in \mathcal{B}^<em>_L$. Therefore, we reduce to <strong>(SP)</strong><br/> [ \min_{y\in\mathcal{Y}}\;\max_{q\in \mathcal{B}^</em><em>L}\; f(y) + \langle q, y\rangle - \sigma</em>{\mathcal{X}}(q). ]</p> <p><strong>Support points.</strong><br/> For $q\in\mathbb{R}^n$, let $s_{\mathcal{X}}(q)\in\arg\max_{x\in\mathcal{X}}\langle q,x\rangle$ be any <em>support point</em> (a subgradient of $\sigma_{\mathcal{X}}$ at $q$). Then $\langle q, s_{\mathcal{X}}(q)-x\rangle\ge 0$ for all $x\in\mathcal{X}$.</p> <hr/> <h2 id="algorithm-1-basic-primaldual-scheme-euclidean-case">Algorithm 1: Basic primal–dual scheme (Euclidean case)</h2> <p>Assume $|\cdot|=|\cdot|<em>2$. With stepsizes $\eta,\alpha&gt;0$ and subgradients $g_t\in\partial f(y_t)$, consider [ y</em>{t+1} = \arg\min_{y\in\mathcal{Y}} \; \langle g_t+q_t,\,y\rangle+\tfrac{\eta}{2}|y-y_t|<em>2^2, ] [ q</em>{t+1} = \arg\min_{q\in \mathcal{B}^*<em>L}\; \langle q,\,s</em>{\mathcal{X}}(q_t)-y_t\rangle+\tfrac{\alpha}{2}|q-q_t|_2^2. ]</p> <p><em>Note.</em> One may remove the explicit constraint $q\in \mathcal{B}^*_L$ if using implicit regularization/penalties that keep $q_t$ bounded.</p> <h3 id="analysis">Analysis</h3> <p>By optimality of $y_{t+1}$ (Moreau envelope inequality), for any $x_<em>\in\mathcal{X}$, [ \langle g_t+q_t,\,y_t-x_</em>\rangle \le \frac{|g_t+q_t|<em>2^2}{2\eta} +\frac{\eta}{2}|x</em><em>-y_t|_2^2 -\frac{\eta}{2}|x_</em>-y_{t+1}|<em>2^2. ] Similarly, by optimality of $q</em>{t+1}$ and any $q_<em>\in \mathcal{B}^</em><em>L$, [ \langle q_t-q</em><em>,\,s_{\mathcal{X}}(q_t)-y_t\rangle \le \frac{|s_{\mathcal{X}}(q_t)-y_t|_2^2}{2\alpha} +\frac{\alpha}{2}|q_</em>-q_t|<em>2^2 -\frac{\alpha}{2}|q</em><em>-q_{t+1}|_2^2. ] Adding, using $\langle q_t, s_{\mathcal{X}}(q_t)-x_</em>\rangle\ge 0$, summing $t=1$ to $T$, and the bounds [ |g_t|<em>2\le L,\quad |s</em>{\mathcal{X}}(q_t)-y_t|<em>2\le D</em>{\mathcal Y},\quad |x_<em>-y_1|_2\le D_{\mathcal X},\quad |q_</em>-q_1|<em>2\le L, ] we get [ \sum</em>{t=1}^T\langle g_t, y_t-x_*\rangle</p> <ul> <li>\sum_{t=1}^T \langle q_*, y_t-s_{\mathcal{X}}(q_t)\rangle \le \frac{T L^2}{\eta}+\frac{\eta D_{\mathcal X}^2}{2} +\frac{T D_{\mathcal Y}^2}{2\alpha}+\frac{\alpha L^2}{2}. ]</li> </ul> <p>By convexity, [ \sum_{t=1}^T\langle g_t, y_t-x_<em>\rangle \ge T\big(f(\bar y_T)-f(x_</em>)\big),\qquad \sum_{t=1}^T \langle q_<em>, y_t-s_{\mathcal{X}}(q_t)\rangle \ge T\langle q_</em>, \bar y_T-\bar x_T\rangle, ] where $\bar y_T=\tfrac1T\sum_{t=1}^T y_t$ and $\bar x_T=\tfrac1T\sum_{t=1}^T s_{\mathcal{X}}(q_t)$. Choosing $q_<em>\in\partial f(\bar x_T)$ with $|q_</em>|\le L$ yields [ f(\bar x_T)-f(x_*) \le \frac{L^2}{\eta} +\frac{\eta D_{\mathcal X}^2}{2T} +\frac{D_{\mathcal Y}^2}{2\alpha} +\frac{\alpha L^2}{2T}. ]</p> <p><strong>Corollary (Rate for Algorithm 1).</strong><br/> Setting $\eta=\sqrt{2}\,L\sqrt{T}/D_{\mathcal X}$ and $\alpha=\sqrt{T}\,D_{\mathcal Y}/L$ gives [ f(\bar x_T)-f(x_*) \;\le\; \frac{(\sqrt{2}\,D_{\mathcal X} + D_{\mathcal Y})\,L}{\sqrt{T}}. ]</p> <hr/> <h2 id="algorithm-2-replacing-d_mathcal-y-by-d_mathcal-x-via-optimistic-omd">Algorithm 2: Replacing $D_{\mathcal Y}$ by $D_{\mathcal X}$ via optimistic OMD</h2> <p>Initialize $y_0\in\mathcal{X}$, set $q_0=0$ so that $s_{\mathcal{X}}(q_0)=y_0$, and let $q_1=q_0$, $y_1=y_0$. For $t\ge 1$, with $g_t\in\partial f(y_t)$, [ y_{t+1} = \arg\min_{y\in\mathcal{Y}} \; \langle g_t+q_t,\,y\rangle+\tfrac{\eta}{2}|y-y_t|<em>2^2, ] [ q</em>{t+1} = \arg\min_{q\in \mathcal{B}^*<em>L}\; \left\langle q,\;2s</em>{\mathcal{X}}(q_t)-s_{\mathcal{X}}(q_{t-1})-y_{t+1}\right\rangle +\tfrac{\alpha}{2}|q-q_t|_2^2. ]</p> <p>This is an <em>optimistic</em> OMD step for the sequence $g^{(q)}<em>t := s</em>{\mathcal{X}}(q_t)-y_t$ with prediction $\tilde g^{(q)}<em>t := s</em>{\mathcal{X}}(q_{t-1})-y_t$ (see, e.g., Orabona, 2023).</p> <h3 id="analysis-1">Analysis</h3> <p>Standard optimistic-OMD arguments give, for any $q_<em>\in\mathcal{B}^</em><em>L$, [ \sum</em>{t=1}^T \langle q_t-q_<em>,\,s_{\mathcal{X}}(q_t)-y_t\rangle \le \sum_{t=1}^T \frac{|s_{\mathcal{X}}(q_t)-s_{\mathcal{X}}(q_{t-1})|_2^2}{2\alpha} +\frac{\alpha}{2}|q_</em>-q_1|<em>2^2. ] Since $s</em>{\mathcal{X}}(q_t)\in\mathcal{X}$ and $q_1=0$, [ \sum_{t=1}^T \langle q_t-q_*,\,s_{\mathcal{X}}(q_t)-y_t\rangle \le \frac{T\,D_{\mathcal X}^2}{2\alpha} + \frac{\alpha L^2}{2}. ]</p> <p>Combining with the same $y$-update bound as in Algorithm\,1 (now using $D_{\mathcal X}$ for both telescoping terms) yields [ \sum_{t=1}^T!\left[ \langle g_t, y_t-x_<em>\rangle +\langle q_t, s_{\mathcal{X}}(q_t)-x_</em>\rangle +\langle q_<em>, y_t-s_{\mathcal{X}}(q_t)\rangle \right] \le \frac{T L^2}{\eta}+\frac{\eta D_{\mathcal X}^2}{2} +\frac{T D_{\mathcal X}^2}{2\alpha}+\frac{\alpha L^2}{2}. ] Using $\langle q_t, s_{\mathcal{X}}(q_t)-x_</em>\rangle\ge 0$ and the same averaging with $q_<em>\in\partial f(\bar x_T)$ gives [ f(\bar x_T)-f(x_</em>) \le \frac{L^2}{\eta} +\frac{\eta D_{\mathcal X}^2}{2T} +\frac{D_{\mathcal X}^2}{2\alpha} +\frac{\alpha L^2}{2T}. ]</p> <p><strong>Corollary (Rate for Algorithm 2).</strong><br/> Choosing $\eta=\sqrt{2}\,L\sqrt{T}/D_{\mathcal X}$ and $\alpha=\sqrt{T}\,D_{\mathcal X}/L$ yields [ f(\bar x_T)-f(x_*) \;\le\; \frac{(\sqrt{2}+1)\,L\,D_{\mathcal X}}{\sqrt{T}}. ]</p> <p><strong>Remark (Oracle model).</strong><br/> The update $s_{\mathcal{X}}(q_t)\in\arg\max_{x\in\mathcal{X}}\langle q_t,x\rangle$ is exactly a linear minimization oracle (LMO) as in Frank–Wolfe methods, hence the scheme is projection-free on $\mathcal{X}$.</p> <hr/> <h2 id="references">References</h2> <ul> <li>Orabona, F. (2023). <em>A Modern Introduction to Online Learning</em>. arXiv preprint.</li> </ul>]]></content><author><name>Kamiar</name></author><category term="optimization"/><category term="convex"/><category term="algorithms"/><category term="projection-free"/><category term="nonsmooth"/><category term="primal-dual"/><category term="OMD"/><summary type="html"><![CDATA[Author: Kamiar Date: February 2024]]></summary></entry></feed>